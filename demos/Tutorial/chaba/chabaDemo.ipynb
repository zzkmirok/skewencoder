{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3629ec1",
   "metadata": {},
   "source": [
    "# Step-by-step tutorial for Loxodynamics\n",
    "\n",
    "Take the workflow of chabazite catalytic system as an example\n",
    "\n",
    "The whole workflow can be separted into 3 main parts:\n",
    "\n",
    "1. **Training**: implemented by pytorch + lightning\n",
    "2. **Generating PLUMED input files**: for both unbiased simulation and biased simulation. \n",
    "3. **Run the simulation**: interface with outer MD drivers.\n",
    "\n",
    "Therefore we need to prepare first some functions for the above usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187b6d8",
   "metadata": {},
   "source": [
    "## Attention! Before starting:\n",
    "\n",
    "One should make sure that the input for MD drivers are available in the identical folder of this script. In CP2K related simulation, both the original `job.inp` and the `job_restart.inp` for retarting are necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb342ac1",
   "metadata": {},
   "source": [
    "We first load the basic modules necessary for the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec0f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from scipy import stats\n",
    "from collections.abc import Sequence\n",
    "from os.path import dirname\n",
    "import subprocess\n",
    "\n",
    "\n",
    "# Locate the script for storage consistencty\n",
    "SCRIPT_DIR = dirname(os.path.abspath(__file__))\n",
    "print(SCRIPT_DIR)\n",
    "sys.path.append(dirname(SCRIPT_DIR)) \n",
    "os.chdir(SCRIPT_DIR)\n",
    "\n",
    "# Based on the OS type determine the CLI env.\n",
    "\n",
    "win_bash_exe_prefix = [\"bash\",\"-c\"]\n",
    "zsh_prefix = [\"/bin/zsh\", \"-c\"]\n",
    "\n",
    "# current_os = \"windows\"\n",
    "current_os = \"zsh\"\n",
    "\n",
    "if current_os == \"windows\":\n",
    "    bash_prefix = win_bash_exe_prefix\n",
    "else:\n",
    "    bash_prefix = zsh_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9478e421",
   "metadata": {},
   "source": [
    "## Customized Workflow\n",
    "> Note: \"Customized\" means that all input descriptors are manually defined. Just as what we did in the Chabazite Demo. \n",
    "\n",
    "We then load the skewencoder related modules that will apply to our system.\n",
    "\n",
    "We also need to define folders storing the trajectories and the trained model in every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8325ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skewencoder.state_detection as STADECT\n",
    "from skewencoder.io import load_dataframe, load_data\n",
    "import skewencoder.switchfunction as sf\n",
    "from skewencoder.model_skewencoder import skewencoder_model_init, skewencoder_model_trainer, skewencoder_model_normalization, cv_eval\n",
    "\n",
    "RESULTS_FOLDER = f\"./results\"\n",
    "UNBIASED_FOLDER = f\"./unbiased\"\n",
    "LIGHTNING_LOGS = f\"./lightning_logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8df8fa",
   "metadata": {},
   "source": [
    "### Preparation: Training procedure.\n",
    "We first define the function for **training** Procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4423b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Trains a model using the Chaba training algorithm with the specified state detection mechanism.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    state_detection : STADECT.State_detection\n",
    "        An instance of the State_detection class that provides methods for detecting states and for decide if apply warm start training strategy.\n",
    "    \n",
    "    iter : int\n",
    "        The number of iterations to run during training. This determines how many times the model will be updated.\n",
    "    \n",
    "    encoder_layers : Sequence[int]\n",
    "        A sequence of integers representing the number of neurons in each layer of the encoder. \n",
    "        This defines the architecture of the encoder network used in training.\n",
    "    \n",
    "    loss_coeff : float\n",
    "        A coefficient used for the weight of skewness loss in the loss function during optimization. \n",
    "        Adjusting this value can influence model performance and convergence behavior.\n",
    "    \n",
    "    batch_size : int\n",
    "        batch size for training.\n",
    "    Returns:\n",
    "    -------\n",
    "    state_detection : STADECT.State_detection\n",
    "        The updated state detection instance after training.\n",
    "        \n",
    "    model : MultiTaskCV\n",
    "        The trained model resulting from the training process (specify type if known).\n",
    "        \n",
    "    ITER_FOLDER : str\n",
    "        The path to the folder containing iteration-related outputs or logs generated during training.\n",
    "        \n",
    "    skewness_dataset : DictDataset\n",
    "        A dataset containing skewness information relevant to the trained model (specify type if known).\n",
    "        \n",
    "    break_flag : bool\n",
    "        A flag indicating whether training was interrupted or completed normally.\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    >>> state_detection, model, ITER_FOLDER, skewness_dataset, break_flag = chaba_training(state_detection, iter, encoder_layers, loss_coeff, batch_size)\n",
    "    \n",
    "    \"\"\"\n",
    "def chaba_training(state_detection: STADECT.State_detection, iter: int, encoder_layers : Sequence[int], loss_coeff: float, batch_size: int):\n",
    "    ITER_FOLDER = RESULTS_FOLDER + f\"/iter_{iter}\"\n",
    "    subprocess.run([*bash_prefix,f\"mkdir {ITER_FOLDER}\"], cwd=SCRIPT_DIR)\n",
    "    break_flag = False\n",
    "\n",
    "    if iter == 0:\n",
    "        filenames_iter = [f\"{UNBIASED_FOLDER}/COLVAR\"]\n",
    "        filenames_all = filenames_iter\n",
    "    else:\n",
    "        filenames_all = [f\"{RESULTS_FOLDER}/iter_{i}/COLVAR\" for i in range(iter) ]\n",
    "        filenames_all.append(f\"{UNBIASED_FOLDER}/COLVAR\")\n",
    "        filenames_iter = [f\"{RESULTS_FOLDER}/iter_{iter-1}/COLVAR\"]\n",
    "    AE_dataset, skewness_dataset, datamodule, _, _ = load_data(filenames_iter,filenames_all,multiple=(iter + 1), bs=batch_size)\n",
    "\n",
    "    if iter == 0:\n",
    "        is_stable_state, is_new_state = state_detection(filenames_iter[0])\n",
    "        model = skewencoder_model_init(AE_dataset,encoder_layers, loss_coeff)\n",
    "    else:\n",
    "        PREV_ITER_FOLDER = f\"{RESULTS_FOLDER}/iter_{iter-1}\" # TODO: Might use os.path.dirname\n",
    "        is_stable_state, is_new_state = state_detection(filenames_iter[0])\n",
    "        apply_warm_start = not is_stable_state\n",
    "        \n",
    "        if not apply_warm_start:\n",
    "            print(\"****************************\")\n",
    "            print(\"Restart from Scratch\")\n",
    "            print(\"Restart from Scratch\")\n",
    "            print(\"Restart from Scratch\")\n",
    "            print(\"****************************\")\n",
    "            model = skewencoder_model_init(AE_dataset,encoder_layers, loss_coeff)\n",
    "        else:\n",
    "            print(\"****************************\")\n",
    "            print(\"Apply Warm Start\")\n",
    "            print(\"Apply Warm Start\")\n",
    "            print(\"Apply Warm Start\")\n",
    "            print(\"****************************\")\n",
    "            model = skewencoder_model_init(AE_dataset,encoder_layers, loss_coeff,iter=iter,PREV_ITER_FOLDER=PREV_ITER_FOLDER)\n",
    "\n",
    "    metrics = skewencoder_model_trainer(model, datamodule, iter_folder=ITER_FOLDER)\n",
    "\n",
    "    model = skewencoder_model_normalization(model, AE_dataset)\n",
    "\n",
    "    traced_model = model.to_torchscript(file_path=f'{ITER_FOLDER}/model_autoencoder_{iter}.pt', method='trace')\n",
    "\n",
    "    return state_detection, model, ITER_FOLDER,skewness_dataset, break_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e838cc",
   "metadata": {},
   "source": [
    "### Preparation: PLUMED input generator.\n",
    "\n",
    "Then we define the PLUMED input files for both unbiased simulation and biased simulation. \n",
    "\n",
    "Note that the input descriptors are manually defined and must be printed in the COLVAR files.\n",
    "\n",
    "In the function `chaba_simulation`, we show how loxodynamics wall is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5c54e30",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SCRIPT_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgen_plumed_chaba_unbiased\u001b[39m(file_path \u001b[38;5;241m=\u001b[39m \u001b[43mSCRIPT_DIR\u001b[49m, simulation_folder \u001b[38;5;241m=\u001b[39m UNBIASED_FOLDER):\n\u001b[0;32m      3\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/plumed.dat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SCRIPT_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "def gen_plumed_chaba_unbiased(file_path = SCRIPT_DIR, simulation_folder = UNBIASED_FOLDER):\n",
    "\n",
    "    file_path = f'{file_path}/plumed.dat'\n",
    "    file = open(file_path, 'w')\n",
    "    input=f'''# vim:ft=plumed\n",
    "UNITS LENGTH=A TIME=0.001  #Amstroeng, hartree, fs\n",
    "# O(BAS): o1: 17, o2: 22, o3: 26, o4: 34,\n",
    "# O(MeOH) o5: 38\n",
    "# H(CH3): h4: 43, h5: 44, h7: 46\n",
    "# H(OH): h2: 39\n",
    "# H(CH2): h3: 42, h6: 45\n",
    "# H(BAS): h1: 37\n",
    "# C: c1: 40, c2: 41\n",
    "# DISTANCES between O(BAS) and H(CH3)\n",
    "o4h7: DISTANCE ATOMS=34,46\n",
    "o4h4: DISTANCE ATOMS=34,43\n",
    "o4h5: DISTANCE ATOMS=34,44\n",
    "\n",
    "o2h7: DISTANCE ATOMS=22,46\n",
    "o2h4: DISTANCE ATOMS=22,43\n",
    "o2h5: DISTANCE ATOMS=22,44\n",
    "\n",
    "o3h7: DISTANCE ATOMS=26,46\n",
    "o3h4: DISTANCE ATOMS=26,43\n",
    "o3h5: DISTANCE ATOMS=26,44\n",
    "\n",
    "o1h7: DISTANCE ATOMS=17,46\n",
    "o1h4: DISTANCE ATOMS=17,43\n",
    "o1h5: DISTANCE ATOMS=17,44\n",
    "\n",
    "# DISTANCES between O(BAS) and H(CH2)\n",
    "o4h3: DISTANCE ATOMS=34,42\n",
    "o4h6: DISTANCE ATOMS=34,45\n",
    "\n",
    "o2h3: DISTANCE ATOMS=22,42\n",
    "o2h6: DISTANCE ATOMS=22,45\n",
    "\n",
    "o3h3: DISTANCE ATOMS=26,42\n",
    "o3h6: DISTANCE ATOMS=26,45\n",
    "\n",
    "o1h3: DISTANCE ATOMS=17,42\n",
    "o1h6: DISTANCE ATOMS=17,45\n",
    "\n",
    "# DISTANCES between O(BAS) and H(OH)\n",
    "o4h2: DISTANCE ATOMS=34,39\n",
    "o2h2: DISTANCE ATOMS=22,39\n",
    "o3h2: DISTANCE ATOMS=26,39\n",
    "o1h2: DISTANCE ATOMS=17,39\n",
    "\n",
    "# DISTANCES between O(BAS) and H(BAS)\n",
    "o4h1: DISTANCE ATOMS=34,37\n",
    "o2h1: DISTANCE ATOMS=22,37\n",
    "o3h1: DISTANCE ATOMS=26,37\n",
    "o1h1: DISTANCE ATOMS=17,37\n",
    "\n",
    "# DISTANCES between O(MeOH) and C\n",
    "o5c1: DISTANCE ATOMS=38,40\n",
    "o5c2: DISTANCE ATOMS=38,41\n",
    "\n",
    "# DISTANCES between O(MeOH) and H(CH3)\n",
    "o5h7: DISTANCE ATOMS=38,46\n",
    "o5h4: DISTANCE ATOMS=38,43\n",
    "o5h5: DISTANCE ATOMS=38,44\n",
    "\n",
    "# DISTANCES between O(MeOH) and H(CH2)\n",
    "o5h3: DISTANCE ATOMS=38,42\n",
    "o5h6: DISTANCE ATOMS=38,45\n",
    "\n",
    "# DISTANCES between O(MeOH) and H(O)\n",
    "o5h1: DISTANCE ATOMS=38,37\n",
    "o5h2: DISTANCE ATOMS=38,39\n",
    "\n",
    "\n",
    "# DISTANCE between atom 7 and 38\n",
    "d17: DISTANCE ATOMS=7,38\n",
    "\n",
    "# Apply upper wall to the distance between 7 and 38\n",
    "uwall: UPPER_WALLS ARG=d17 AT=3.5 KAPPA=200.0\n",
    "\n",
    "# PRINT all variables\n",
    "\n",
    "PRINT FMT=%g STRIDE=10 FILE={simulation_folder}/COLVAR ARG=o4h7,o4h4,o4h5,o2h7,o2h4,o2h5,o3h7,o3h4,o3h5,o1h7,o1h4,o1h5,o4h3,o4h6,o2h3,o2h6,o3h3,o3h6,o1h3,o1h6,o4h2,o2h2,o3h2,o1h2,o4h1,o2h1,o3h1,o1h1,o5c1,o5c2,o5h7,o5h4,o5h5,o5h3,o5h6,o5h1,o5h2'''\n",
    "    print(input, file=file)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "\n",
    "def gen_plumed_chaba_biased(model_name : str,\n",
    "                         file_path : str,\n",
    "                         simulation_folder,\n",
    "                         pos,\n",
    "                         skew,\n",
    "                         kappa,\n",
    "                         offset):\n",
    "\n",
    "    file_path = f'{file_path}/plumed.dat'\n",
    "    file = open(file_path, 'w')\n",
    "    input=f'''# vim:ft=plumed\n",
    "UNITS LENGTH=A TIME=0.001  #Amstroeng, hartree, fs\n",
    "# O(BAS): o1: 17, o2: 22, o3: 26, o4: 34,\n",
    "# O(MeOH) o5: 38\n",
    "# H(CH3): h4: 43, h5: 44, h7: 46\n",
    "# H(OH): h2: 39\n",
    "# H(CH2): h3: 42, h6: 45\n",
    "# H(BAS): h1: 37\n",
    "# C: c1: 40, c2: 41\n",
    "# DISTANCES between O(BAS) and H(CH3)\n",
    "o4h7: DISTANCE ATOMS=34,46\n",
    "o4h4: DISTANCE ATOMS=34,43\n",
    "o4h5: DISTANCE ATOMS=34,44\n",
    "\n",
    "o2h7: DISTANCE ATOMS=22,46\n",
    "o2h4: DISTANCE ATOMS=22,43\n",
    "o2h5: DISTANCE ATOMS=22,44\n",
    "\n",
    "o3h7: DISTANCE ATOMS=26,46\n",
    "o3h4: DISTANCE ATOMS=26,43\n",
    "o3h5: DISTANCE ATOMS=26,44\n",
    "\n",
    "o1h7: DISTANCE ATOMS=17,46\n",
    "o1h4: DISTANCE ATOMS=17,43\n",
    "o1h5: DISTANCE ATOMS=17,44\n",
    "\n",
    "# DISTANCES between O(BAS) and H(CH2)\n",
    "o4h3: DISTANCE ATOMS=34,42\n",
    "o4h6: DISTANCE ATOMS=34,45\n",
    "\n",
    "o2h3: DISTANCE ATOMS=22,42\n",
    "o2h6: DISTANCE ATOMS=22,45\n",
    "\n",
    "o3h3: DISTANCE ATOMS=26,42\n",
    "o3h6: DISTANCE ATOMS=26,45\n",
    "\n",
    "o1h3: DISTANCE ATOMS=17,42\n",
    "o1h6: DISTANCE ATOMS=17,45\n",
    "\n",
    "# DISTANCES between O(BAS) and H(OH)\n",
    "o4h2: DISTANCE ATOMS=34,39\n",
    "o2h2: DISTANCE ATOMS=22,39\n",
    "o3h2: DISTANCE ATOMS=26,39\n",
    "o1h2: DISTANCE ATOMS=17,39\n",
    "\n",
    "# DISTANCES between O(BAS) and H(BAS)\n",
    "o4h1: DISTANCE ATOMS=34,37\n",
    "o2h1: DISTANCE ATOMS=22,37\n",
    "o3h1: DISTANCE ATOMS=26,37\n",
    "o1h1: DISTANCE ATOMS=17,37\n",
    "\n",
    "# DISTANCES between O(MeOH) and C\n",
    "o5c1: DISTANCE ATOMS=38,40\n",
    "o5c2: DISTANCE ATOMS=38,41\n",
    "\n",
    "# DISTANCES between O(MeOH) and H(CH3)\n",
    "o5h7: DISTANCE ATOMS=38,46\n",
    "o5h4: DISTANCE ATOMS=38,43\n",
    "o5h5: DISTANCE ATOMS=38,44\n",
    "\n",
    "# DISTANCES between O(MeOH) and H(CH2)\n",
    "o5h3: DISTANCE ATOMS=38,42\n",
    "o5h6: DISTANCE ATOMS=38,45\n",
    "\n",
    "# DISTANCES between O(MeOH) and H(O)\n",
    "o5h1: DISTANCE ATOMS=38,37\n",
    "o5h2: DISTANCE ATOMS=38,39\n",
    "\n",
    "\n",
    "# DISTANCE between atom 7 and 38\n",
    "d17: DISTANCE ATOMS=7,38\n",
    "\n",
    "# Apply upper wall to the distance between 7 and 38\n",
    "uwall: UPPER_WALLS ARG=d17 AT=3.5 KAPPA=200.0\n",
    "cv: PYTORCH_MODEL FILE={model_name} ARG=o4h7,o4h4,o4h5,o2h7,o2h4,o2h5,o3h7,o3h4,o3h5,o1h7,o1h4,o1h5,o4h3,o4h6,o2h3,o2h6,o3h3,o3h6,o1h3,o1h6,o4h2,o2h2,o3h2,o1h2,o4h1,o2h1,o3h1,o1h1,o5c1,o5c2,o5h7,o5h4,o5h5,o5h3,o5h6,o5h1,o5h2\n",
    "\n",
    "# UPPER_WALLS ARG=c1c2 AT=+8.5 KAPPA=250.0 EXP=2 LABEL=constr_c1c2 # Wall for potential constraints\n",
    "    '''\n",
    "    print(input, file=file)\n",
    "    file.close()\n",
    "    walltype=\"\"\n",
    "    if skew < 0:\n",
    "        walltype = \"UPPER_WALLS\"\n",
    "        offset = -offset\n",
    "    else:\n",
    "        walltype = \"LOWER_WALLS\"\n",
    "    with open(file_path,\"a\") as f:\n",
    "        print(f\"\"\"\n",
    "# Energy wall for aes cv\n",
    "wall: {walltype} ARG=cv.node-0 AT={pos+offset} KAPPA={kappa} ExP=2 EPS=1 OFFSET=0.0\n",
    "PRINT FMT=%g STRIDE=10 FILE={simulation_folder}/COLVAR ARG=o4h7,o4h4,o4h5,o2h7,o2h4,o2h5,o3h7,o3h4,o3h5,o1h7,o1h4,o1h5,o4h3,o4h6,o2h3,o2h6,o3h3,o3h6,o1h3,o1h6,o4h2,o2h2,o3h2,o1h2,o4h1,o2h1,o3h1,o1h1,o5c1,o5c2,o5h7,o5h4,o5h5,o5h3,o5h6,o5h1,o5h2,cv.*\"\"\",file=f)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def chaba_simulation(iter_folder, model_name, model, dataset, kappa, offset):\n",
    "    nn_output = cv_eval(model, dataset).flatten()\n",
    "    mu_sknn = np.mean(nn_output)\n",
    "    var_sknn = np.var(nn_output)\n",
    "    skew_sknn = stats.skew(nn_output)\n",
    "    offset += np.sqrt(var_sknn)\n",
    "    gen_plumed_chaba_biased(model_name=model_name,\n",
    "                         file_path=\".\",\n",
    "                         simulation_folder=iter_folder,\n",
    "                         pos=mu_sknn,\n",
    "                         skew=skew_sknn,\n",
    "                         kappa=kappa,\n",
    "                         offset=offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125cfdaa",
   "metadata": {},
   "source": [
    "### Main Workflow\n",
    "\n",
    "Now we can start our main work flow.\n",
    "\n",
    "1. Parameter initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0ce522",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa = 500\n",
    "\n",
    "n_max_iter = 2\n",
    "loss_coeff = 0.1\n",
    "torch.manual_seed(22)\n",
    "batch_size = 100\n",
    "offset = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c9bc67",
   "metadata": {},
   "source": [
    "2. Clear the history data. \n",
    "\n",
    "**Don't run the following snippet in the Demo.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c146b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241m.\u001b[39mrun([\u001b[38;5;241m*\u001b[39mbash_prefix,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrm -rf \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRESULTS_FOLDER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m], cwd\u001b[38;5;241m=\u001b[39mSCRIPT_DIR)\n\u001b[0;32m      2\u001b[0m subprocess\u001b[38;5;241m.\u001b[39mrun([\u001b[38;5;241m*\u001b[39mbash_prefix,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrm -rf \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLIGHTNING_LOGS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m], cwd\u001b[38;5;241m=\u001b[39mSCRIPT_DIR)\n\u001b[0;32m      3\u001b[0m subprocess\u001b[38;5;241m.\u001b[39mrun([\u001b[38;5;241m*\u001b[39mbash_prefix,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrm -rf \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mUNBIASED_FOLDER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m], cwd\u001b[38;5;241m=\u001b[39mSCRIPT_DIR)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'subprocess' is not defined"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN.\n",
    "subprocess.run([*bash_prefix,f\"rm -rf {RESULTS_FOLDER}\"], cwd=SCRIPT_DIR)\n",
    "subprocess.run([*bash_prefix,f\"rm -rf {LIGHTNING_LOGS}\"], cwd=SCRIPT_DIR)\n",
    "subprocess.run([*bash_prefix,f\"rm -rf {UNBIASED_FOLDER}\"], cwd=SCRIPT_DIR)\n",
    "subprocess.run([*bash_prefix, f\"rm -f {kappa}_iter* all*.pdb\"], cwd=SCRIPT_DIR)\n",
    "\n",
    "subprocess.run([*bash_prefix, f\"mkdir -p {UNBIASED_FOLDER}\"])\n",
    "\n",
    "subprocess.run([*bash_prefix, f\"echo '******************************************************'\"], cwd=SCRIPT_DIR)\n",
    "subprocess.run([*bash_prefix, f\"echo Start unbiased simulation\"], cwd=SCRIPT_DIR)\n",
    "subprocess.run([*bash_prefix, f\"echo '******************************************************'\"], cwd=SCRIPT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1354780",
   "metadata": {},
   "source": [
    "3. Generate PLUMED input file for unbiased simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0702152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_plumed_chaba_unbiased()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05df718",
   "metadata": {},
   "source": [
    "4. Run unbiased simulation using CP2K.\n",
    "\n",
    "**Don't run the following snippet in the Demo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae7c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([*bash_prefix,\"cp2k.popt job.inp > output.log\"], cwd=SCRIPT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f37bf",
   "metadata": {},
   "source": [
    "5. Organizing data for further usage.\n",
    "\n",
    "**Don't run the following snippet in the Demo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([*bash_prefix, \"mv Chaba-1.restart newiter.restart\"], cwd = SCRIPT_DIR)\n",
    "subprocess.run([*bash_prefix, \"rm -f Chaba*.restart\"], cwd=SCRIPT_DIR)\n",
    "subprocess.run([*bash_prefix, f\"mv Chaba-pos-1.pdb {kappa}_iteration_Chaba_unbiased-pos.pdb\"], cwd = SCRIPT_DIR)\n",
    "subprocess.run([*bash_prefix, f\"cat {kappa}_iteration_Chaba_unbiased-pos.pdb > all_{kappa}.pdb\"], cwd=SCRIPT_DIR)\n",
    "subprocess.run([*bash_prefix,\"rm -f PLUMED.OUT Chaba*\"], cwd=SCRIPT_DIR)\n",
    "subprocess.run([*bash_prefix, f\"mkdir -p {RESULTS_FOLDER}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11f6324",
   "metadata": {},
   "source": [
    "6. Parse the initial unbiased sampling for automatically determining input descriptors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4096424",
   "metadata": {},
   "outputs": [],
   "source": [
    "bond_type_dict, n_descriptors,heavy_atom_pairs_list = STADECT.parse_unbiased_colvar(colvar_file = f\"{UNBIASED_FOLDER}/COLVAR\")\n",
    "encoder_layers = [n_descriptors, 90, 40, 20, 5, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a01173",
   "metadata": {},
   "source": [
    "7. Initialize state detection object.\n",
    "\n",
    "   This object will save the current and historical states traversed by the system during the simulation.\n",
    "   \n",
    "   The initial state must be stable. Ohterwise further thermalization is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7408ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_detection = STADECT.State_detection((0.3, 0.7), bond_type_dict=bond_type_dict, n_heavy_atom_pairs=n_descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58c1aec",
   "metadata": {},
   "source": [
    "### Biased Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01de1662",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_max_iter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mn_max_iter\u001b[49m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Train the current model\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     state_detection, model, ITER_FOLDER, skewness_dataset, break_flag \u001b[38;5;241m=\u001b[39m chaba_training(state_detection, \u001b[38;5;28miter\u001b[39m, encoder_layers, loss_coeff, batch_size)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# For logging\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_max_iter' is not defined"
     ]
    }
   ],
   "source": [
    "for iter in range(n_max_iter):\n",
    "    # Train the current model\n",
    "    state_detection, model, ITER_FOLDER, skewness_dataset, break_flag = chaba_training(state_detection, iter, encoder_layers, loss_coeff, batch_size)\n",
    "    # For logging\n",
    "    subprocess.run([*bash_prefix, f\"echo '******************************************************'\"], cwd=SCRIPT_DIR)\n",
    "    subprocess.run([*bash_prefix, f\"echo At the iteration {iter} training step, \"], cwd=SCRIPT_DIR)\n",
    "    subprocess.run([*bash_prefix, f\"echo The current state is {state_detection.current_state}\"], cwd=SCRIPT_DIR)\n",
    "    subprocess.run([*bash_prefix, f\"echo '******************************************************'\"], cwd=SCRIPT_DIR)\n",
    "\n",
    "    # Generate PLUMED input files for biased simulation\n",
    "    model_name = f\"{ITER_FOLDER}/model_autoencoder_{iter}.pt\"\n",
    "    chaba_simulation(ITER_FOLDER, model_name, model, skewness_dataset, kappa, offset)\n",
    "    \n",
    "    # Run Biased Simulation\n",
    "    subprocess.run([*bash_prefix,\"cp2k.popt job_restart.inp > output.log\"], cwd=SCRIPT_DIR)\n",
    "\n",
    "    # Organize trajectories\n",
    "    subprocess.run([*bash_prefix, \"mv Chaba-1.restart newiter.restart\"], cwd = SCRIPT_DIR)\n",
    "    subprocess.run([*bash_prefix, \"rm -f Chaba*.restart\"], cwd=SCRIPT_DIR)\n",
    "    subprocess.run([*bash_prefix, f\"mv Chaba-pos-1.pdb {kappa}_iteration_Chaba_{iter}-pos.pdb\"], cwd = SCRIPT_DIR)\n",
    "    subprocess.run([*bash_prefix, f\"tail -n +3 {kappa}_iteration_Chaba_{iter}-pos.pdb >> all_{kappa}.pdb\"], cwd=SCRIPT_DIR)\n",
    "    subprocess.run([*bash_prefix,\"rm -f PLUMED.OUT Chaba*\"], cwd=SCRIPT_DIR)\n",
    "\n",
    "    # logging\n",
    "    subprocess.run([*bash_prefix, f\"echo CP2K simulation at iteration {iter} with plumed ends\"], cwd=SCRIPT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5714dd94",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state_detection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthere are in total \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mstate_detection\u001b[49m\u001b[38;5;241m.\u001b[39mn_states\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m states\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'state_detection' is not defined"
     ]
    }
   ],
   "source": [
    "# Show how many states are traversed by the simulation\n",
    "print(f\"there are in total {state_detection.n_states} states\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cty-mlcolvar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
