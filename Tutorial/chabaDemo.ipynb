{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a60396",
   "metadata": {},
   "source": [
    "# Preparation Phase for the Tutorial\n",
    "\n",
    "In the following steps we first get the system and package configuration ready for the tutorial.\n",
    "\n",
    "## 1.  External MD Driver Requirement Checking\n",
    "CP2K patched with PLUMED (PLUMED enabled NN)\n",
    "\n",
    "If a certain package is missing, the rest of the notebook **cannot run properly**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902cf6ee-92bf-4959-b7bd-e231c20f4428",
   "metadata": {},
   "outputs": [],
   "source": [
    "!which plumed\n",
    "!which cp2k.popt\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1a643f-14be-498d-b32d-d2c28755be0c",
   "metadata": {},
   "source": [
    "## 2. Skewencoder Installation\n",
    "Create a python virtual env for this tutorial if necessary.\n",
    "\n",
    "The python package `skewencoder` will be installed in the certain python venv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d47a941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the path to your virtual environment\n",
    "venv_dir = \"tutorial4loxodynamics\"\n",
    "\n",
    "# Check if the virtual environment directory exists\n",
    "if os.path.exists(venv_dir):\n",
    "    print(f\"Virtual environment '{venv_dir}' for this tutorial already exists.\")\n",
    "else:\n",
    "    print(f\"Virtual environment '{venv_dir}' for this tutorial does not exist. Creating now...\")\n",
    "    !python -m venv {venv_dir}\n",
    "    print(f\"Virtual environment '{venv_dir}' for this tutorial created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63088471-19db-4947-8f91-a41356989d2e",
   "metadata": {},
   "source": [
    "Install ipykernel in the created python venv for this notebook\n",
    "\n",
    "`pip install pytz python-dateutil` may not be necessary because it is specifically for the Python distribution on RWTH Cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6436c2-730b-4bb3-ab5f-2dc39357d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ipykernel in the virtual environment\n",
    "!source {venv_dir}/bin/activate\n",
    "\n",
    "!{venv_dir}/bin/pip install pytz python-dateutil\n",
    "\n",
    "!{venv_dir}/bin/pip install ipykernel\n",
    "\n",
    "!{venv_dir}/bin/python -m ipykernel install --user --name={venv_dir} --display-name=\"Python ({venv_dir})\"\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# Define your desired kernel name\n",
    "desired_kernel_name = venv_dir\n",
    "\n",
    "# Get the list of installed kernels\n",
    "result = subprocess.run(['jupyter', 'kernelspec', 'list'], stdout=subprocess.PIPE)\n",
    "\n",
    "# Decode and split the result into lines\n",
    "kernels_list = result.stdout.decode('utf-8').split('\\n')\n",
    "\n",
    "# Check if the desired kernel name is in the list\n",
    "kernel_exists = any(desired_kernel_name in line for line in kernels_list)\n",
    "\n",
    "if kernel_exists:\n",
    "    print(f\"Kernel '{desired_kernel_name}' already exists.\")\n",
    "else:\n",
    "    print(f\"Kernel '{desired_kernel_name}' does not exist. Proceeding with installation...\")\n",
    "    # Run your installation command here\n",
    "    install_command = f\"!{venv_dir}/bin/python -m ipykernel install --user --name={desired_kernel_name} --display-name='Python ({desired_kernel_name})'\"\n",
    "    exec(install_command)\n",
    "\n",
    "print(f\"Kernel added for virtual environment {venv_dir}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99abf4f-642b-4ee0-b7eb-92dde5dcd654",
   "metadata": {},
   "source": [
    "**Mandatory: Switch the ipykernel to the corresponding virtual env MANUALLY**\n",
    "\n",
    "Select the kernel named `Python ({your_venv_dir})` in the kernel menu.\n",
    "\n",
    "Run the following cell to check if the python exec and pip are successfully switched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d414bed-f178-4a33-bec3-e3481248977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "if sys.prefix != sys.base_prefix:\n",
    "    print(\"Kernel switched to venv. Skewencoder will be installed in the default virtual environment.\")\n",
    "    virtual_env = sys.prefix\n",
    "    # Set the VIRTUAL_ENV environment variable\n",
    "    os.environ['VIRTUAL_ENV'] = virtual_env\n",
    "    # Prepend the virtual environment's bin directory to PATH\n",
    "    os.environ['PATH'] = f\"{virtual_env}/bin:\" + os.environ['PATH']\n",
    "else:\n",
    "    print(\"Kernel wasn't switched. Skewencoder might be installed globally.\")\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# Get site-packages directory using pip show command\n",
    "result = subprocess.run(['pip', 'show', 'pip'], stdout=subprocess.PIPE)\n",
    "output = result.stdout.decode()\n",
    "\n",
    "for line in output.split('\\n'):\n",
    "    if line.startswith('Location'):\n",
    "        print(f\"Packages will be installed at: {line.split(': ')[1]}\")\n",
    "\n",
    "!which pip\n",
    "!which python\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current working directory for tutorial:\", current_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c917f-d4f4-4eb9-bc5b-f50ce92d0b60",
   "metadata": {},
   "source": [
    "**Install skewencoder**\n",
    "\n",
    "If the current working dir is **still** the copied `Tutorial` folder, one can directly run the following command to install skewencoder and the dependencies.\n",
    "\n",
    "If the current working dir is **not** the copied `Tutorial` folder, one should manually set the path to the `skewencoder` repository and then install.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40f07d-34c6-4a3a-b459-6a9de15a4676",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_to_SKEWENCODER_Rep = \"../skewencoder\"\n",
    "\n",
    "absolute_path = os.path.abspath(PATH_to_SKEWENCODER_Rep)\n",
    "\n",
    "def is_package_installed(package_name):\n",
    "    # Get the list of installed packages\n",
    "    result = subprocess.run(['pip', 'list'], stdout=subprocess.PIPE)\n",
    "    pip_list_output = result.stdout.decode()\n",
    "\n",
    "    # Check if the package is in the list\n",
    "    return package_name.lower() in pip_list_output.lower()\n",
    "\n",
    "package_name = \"skewencoder\"\n",
    "\n",
    "# Check if the package is installed\n",
    "needs_installation = not is_package_installed(package_name)\n",
    "\n",
    "if needs_installation:\n",
    "    print(f\"{package_name} is not installed. Needs installation.\")\n",
    "    print(f\"Start to install {package_name}...\")\n",
    "    !pip install -e {PATH_to_SKEWENCODER_Rep}\n",
    "else:\n",
    "    print(f\"{package_name} is already installed.\")\n",
    "\n",
    "sys.path.append(absolute_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187b6d8",
   "metadata": {},
   "source": [
    "# Attention! Before starting:\n",
    "\n",
    "One should make sure that the input for MD drivers are available in the identical folder of this script. \n",
    "\n",
    "In CP2K related simulation, both the original `job.inp` and the `job_restart.inp` for retarting are necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57e148f-c5ce-4789-aed7-3833c9309305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file names\n",
    "file_names = [\"job.inp\", \"job_restart.inp\"]\n",
    "\n",
    "# Check if each file exists in the current directory\n",
    "files_exist = {file_name: os.path.isfile(file_name) for file_name in file_names}\n",
    "\n",
    "# Print results\n",
    "for file_name, exists in files_exist.items():\n",
    "    if exists:\n",
    "        print(f\"{file_name} is present in the current folder.\")\n",
    "    else:\n",
    "        print(f\"{file_name} is not present in the current folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3629ec1",
   "metadata": {},
   "source": [
    "# Step-by-step tutorial for Loxodynamics\n",
    "\n",
    "Take the workflow of chabazite catalytic system as an example\n",
    "\n",
    "The whole workflow can be separted into 3 main parts:\n",
    "\n",
    "1. **Training**: implemented by pytorch + lightning\n",
    "2. **Generating PLUMED input files**: for both unbiased simulation and biased simulation. \n",
    "3. **Run the simulation**: interface with outer MD drivers.\n",
    "\n",
    "Therefore we need to prepare first some functions for the above usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb342ac1",
   "metadata": {},
   "source": [
    "We first load the basic modules necessary for the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec0f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy import stats\n",
    "from collections.abc import Sequence\n",
    "\n",
    "# Locate the script for storage consistencty\n",
    "SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "# Based on the OS type determine the CLI env.\n",
    "\n",
    "win_bash_exe_prefix = [\"bash\",\"-c\"]\n",
    "zsh_prefix = [\"/bin/zsh\", \"-c\"]\n",
    "\n",
    "# current_os = \"windows\"\n",
    "current_os = \"zsh\"\n",
    "\n",
    "if current_os == \"windows\":\n",
    "    bash_prefix = win_bash_exe_prefix\n",
    "else:\n",
    "    bash_prefix = zsh_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9478e421",
   "metadata": {},
   "source": [
    "**We will demonstrate in the following a Customized Workflow in the following tutorial**\n",
    "> Note: \"Customized\" means that all input descriptors are manually defined. Just as what we did in the Chabazite Demo. \n",
    "\n",
    "We then load the skewencoder related modules that will apply to our system.\n",
    "\n",
    "We also need to define folders storing the trajectories and the trained model in every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8325ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skewencoder.state_detection as STADECT\n",
    "from skewencoder.io import load_dataframe, load_data\n",
    "import skewencoder.switchfunction as sf\n",
    "from skewencoder.model_skewencoder import skewencoder_model_init, skewencoder_model_trainer, skewencoder_model_normalization, cv_eval\n",
    "\n",
    "RESULTS_FOLDER = f\"./results\"\n",
    "UNBIASED_FOLDER = f\"./unbiased\"\n",
    "LIGHTNING_LOGS = f\"./lightning_logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8df8fa",
   "metadata": {},
   "source": [
    "### 1. Preparation: Training procedure.\n",
    "We first define the function for **training** Procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4423b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Trains a model using the Chaba training algorithm with the specified state detection mechanism.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    state_detection : STADECT.State_detection\n",
    "        An instance of the State_detection class that provides methods for detecting states and for decide if apply warm start training strategy.\n",
    "    \n",
    "    iter : int\n",
    "        The number of iterations to run during training. This determines how many times the model will be updated.\n",
    "    \n",
    "    encoder_layers : Sequence[int]\n",
    "        A sequence of integers representing the number of neurons in each layer of the encoder. \n",
    "        This defines the architecture of the encoder network used in training.\n",
    "    \n",
    "    loss_coeff : float\n",
    "        A coefficient used for the weight of skewness loss in the loss function during optimization. \n",
    "        Adjusting this value can influence model performance and convergence behavior.\n",
    "    \n",
    "    batch_size : int\n",
    "        batch size for training.\n",
    "    Returns:\n",
    "    -------\n",
    "    state_detection : STADECT.State_detection\n",
    "        The updated state detection instance after training.\n",
    "        \n",
    "    model : MultiTaskCV\n",
    "        The trained model resulting from the training process (specify type if known).\n",
    "        \n",
    "    ITER_FOLDER : str\n",
    "        The path to the folder containing iteration-related outputs or logs generated during training.\n",
    "        \n",
    "    skewness_dataset : DictDataset\n",
    "        A dataset containing skewness information relevant to the trained model (specify type if known).\n",
    "        \n",
    "    break_flag : bool\n",
    "        A flag indicating whether training was interrupted or completed normally.\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    >>> state_detection, model, ITER_FOLDER, skewness_dataset, break_flag = chaba_training(state_detection, iter, encoder_layers, loss_coeff, batch_size)\n",
    "    \n",
    "    \"\"\"\n",
    "def chaba_training(state_detection: STADECT.State_detection, iter: int, encoder_layers : Sequence[int], loss_coeff: float, batch_size: int):\n",
    "    ITER_FOLDER = RESULTS_FOLDER + f\"/iter_{iter}\"\n",
    "    subprocess.run([*bash_prefix,f\"mkdir {ITER_FOLDER}\"], cwd=SCRIPT_DIR)\n",
    "    break_flag = False\n",
    "\n",
    "    if iter == 0:\n",
    "        filenames_iter = [f\"{UNBIASED_FOLDER}/COLVAR\"]\n",
    "        filenames_all = filenames_iter\n",
    "    else:\n",
    "        filenames_all = [f\"{RESULTS_FOLDER}/iter_{i}/COLVAR\" for i in range(iter) ]\n",
    "        filenames_all.append(f\"{UNBIASED_FOLDER}/COLVAR\")\n",
    "        filenames_iter = [f\"{RESULTS_FOLDER}/iter_{iter-1}/COLVAR\"]\n",
    "    AE_dataset, skewness_dataset, datamodule, _, _ = load_data(filenames_iter,filenames_all,multiple=(iter + 1), bs=batch_size)\n",
    "\n",
    "    if iter == 0:\n",
    "        is_stable_state, is_new_state = state_detection(filenames_iter[0])\n",
    "        model = skewencoder_model_init(AE_dataset,encoder_layers, loss_coeff)\n",
    "    else:\n",
    "        PREV_ITER_FOLDER = f\"{RESULTS_FOLDER}/iter_{iter-1}\" # TODO: Might use os.path.dirname\n",
    "        is_stable_state, is_new_state = state_detection(filenames_iter[0])\n",
    "        apply_warm_start = not is_stable_state\n",
    "        \n",
    "        if not apply_warm_start:\n",
    "            print(\"****************************\")\n",
    "            print(\"Restart from Scratch\")\n",
    "            print(\"Restart from Scratch\")\n",
    "            print(\"Restart from Scratch\")\n",
    "            print(\"****************************\")\n",
    "            model = skewencoder_model_init(AE_dataset,encoder_layers, loss_coeff)\n",
    "        else:\n",
    "            print(\"****************************\")\n",
    "            print(\"Apply Warm Start\")\n",
    "            print(\"Apply Warm Start\")\n",
    "            print(\"Apply Warm Start\")\n",
    "            print(\"****************************\")\n",
    "            model = skewencoder_model_init(AE_dataset,encoder_layers, loss_coeff,iter=iter,PREV_ITER_FOLDER=PREV_ITER_FOLDER)\n",
    "\n",
    "    metrics = skewencoder_model_trainer(model, datamodule, iter_folder=ITER_FOLDER)\n",
    "\n",
    "    model = skewencoder_model_normalization(model, AE_dataset)\n",
    "\n",
    "    traced_model = model.to_torchscript(file_path=f'{ITER_FOLDER}/model_autoencoder_{iter}.pt', method='trace')\n",
    "\n",
    "    return state_detection, model, ITER_FOLDER,skewness_dataset, break_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e838cc",
   "metadata": {},
   "source": [
    "### 2. Preparation: PLUMED input generator.\n",
    "\n",
    "Then we define the PLUMED input files for both **unbiased** simulation and **biased** simulation. \n",
    "\n",
    "Note that the input descriptors are manually defined and must be printed in the COLVAR files.\n",
    "\n",
    "In the function `chaba_simulation`, we show how loxodynamics wall is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c54e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_plumed_chaba_unbiased(file_path = SCRIPT_DIR, simulation_folder = UNBIASED_FOLDER):\n",
    "\n",
    "    file_path = f'{file_path}/plumed.dat'\n",
    "    file = open(file_path, 'w')\n",
    "    input=f'''# vim:ft=plumed\n",
    "UNITS LENGTH=A TIME=0.001  #Amstroeng, hartree, fs\n",
    "# O(BAS): o1: 17, o2: 22, o3: 26, o4: 34,\n",
    "# O(MeOH) o5: 38\n",
    "# H(CH3): h4: 43, h5: 44, h7: 46\n",
    "# H(OH): h2: 39\n",
    "# H(CH2): h3: 42, h6: 45\n",
    "# H(BAS): h1: 37\n",
    "# C: c1: 40, c2: 41\n",
    "# DISTANCES between O(BAS) and H(CH3)\n",
    "o4h7: DISTANCE ATOMS=34,46\n",
    "o4h4: DISTANCE ATOMS=34,43\n",
    "o4h5: DISTANCE ATOMS=34,44\n",
    "\n",
    "o2h7: DISTANCE ATOMS=22,46\n",
    "o2h4: DISTANCE ATOMS=22,43\n",
    "o2h5: DISTANCE ATOMS=22,44\n",
    "\n",
    "o3h7: DISTANCE ATOMS=26,46\n",
    "o3h4: DISTANCE ATOMS=26,43\n",
    "o3h5: DISTANCE ATOMS=26,44\n",
    "\n",
    "o1h7: DISTANCE ATOMS=17,46\n",
    "o1h4: DISTANCE ATOMS=17,43\n",
    "o1h5: DISTANCE ATOMS=17,44\n",
    "\n",
    "# DISTANCES between O(BAS) and H(CH2)\n",
    "o4h3: DISTANCE ATOMS=34,42\n",
    "o4h6: DISTANCE ATOMS=34,45\n",
    "\n",
    "o2h3: DISTANCE ATOMS=22,42\n",
    "o2h6: DISTANCE ATOMS=22,45\n",
    "\n",
    "o3h3: DISTANCE ATOMS=26,42\n",
    "o3h6: DISTANCE ATOMS=26,45\n",
    "\n",
    "o1h3: DISTANCE ATOMS=17,42\n",
    "o1h6: DISTANCE ATOMS=17,45\n",
    "\n",
    "# DISTANCES between O(BAS) and H(OH)\n",
    "o4h2: DISTANCE ATOMS=34,39\n",
    "o2h2: DISTANCE ATOMS=22,39\n",
    "o3h2: DISTANCE ATOMS=26,39\n",
    "o1h2: DISTANCE ATOMS=17,39\n",
    "\n",
    "# DISTANCES between O(BAS) and H(BAS)\n",
    "o4h1: DISTANCE ATOMS=34,37\n",
    "o2h1: DISTANCE ATOMS=22,37\n",
    "o3h1: DISTANCE ATOMS=26,37\n",
    "o1h1: DISTANCE ATOMS=17,37\n",
    "\n",
    "# DISTANCES between O(MeOH) and C\n",
    "o5c1: DISTANCE ATOMS=38,40\n",
    "o5c2: DISTANCE ATOMS=38,41\n",
    "\n",
    "# DISTANCES between O(MeOH) and H(CH3)\n",
    "o5h7: DISTANCE ATOMS=38,46\n",
    "o5h4: DISTANCE ATOMS=38,43\n",
    "o5h5: DISTANCE ATOMS=38,44\n",
    "\n",
    "# DISTANCES between O(MeOH) and H(CH2)\n",
    "o5h3: DISTANCE ATOMS=38,42\n",
    "o5h6: DISTANCE ATOMS=38,45\n",
    "\n",
    "# DISTANCES between O(MeOH) and H(O)\n",
    "o5h1: DISTANCE ATOMS=38,37\n",
    "o5h2: DISTANCE ATOMS=38,39\n",
    "\n",
    "\n",
    "# DISTANCE between atom 7 and 38\n",
    "d17: DISTANCE ATOMS=7,38\n",
    "\n",
    "# Apply upper wall to the distance between 7 and 38\n",
    "uwall: UPPER_WALLS ARG=d17 AT=3.5 KAPPA=200.0\n",
    "\n",
    "# PRINT all variables\n",
    "\n",
    "PRINT FMT=%g STRIDE=10 FILE={simulation_folder}/COLVAR ARG=o4h7,o4h4,o4h5,o2h7,o2h4,o2h5,o3h7,o3h4,o3h5,o1h7,o1h4,o1h5,o4h3,o4h6,o2h3,o2h6,o3h3,o3h6,o1h3,o1h6,o4h2,o2h2,o3h2,o1h2,o4h1,o2h1,o3h1,o1h1,o5c1,o5c2,o5h7,o5h4,o5h5,o5h3,o5h6,o5h1,o5h2'''\n",
    "    print(input, file=file)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "\n",
    "def gen_plumed_chaba_biased(model_name : str,\n",
    "                         file_path : str,\n",
    "                         simulation_folder,\n",
    "                         pos,\n",
    "                         skew,\n",
    "                         kappa,\n",
    "                         offset):\n",
    "\n",
    "    file_path = f'{file_path}/plumed.dat'\n",
    "    file = open(file_path, 'w')\n",
    "    input=f'''# vim:ft=plumed\n",
    "UNITS LENGTH=A TIME=0.001  #Amstroeng, hartree, fs\n",
    "# O(BAS): o1: 17, o2: 22, o3: 26, o4: 34,\n",
    "# O(MeOH) o5: 38\n",
    "# H(CH3): h4: 43, h5: 44, h7: 46\n",
    "# H(OH): h2: 39\n",
    "# H(CH2): h3: 42, h6: 45\n",
    "# H(BAS): h1: 37\n",
    "# C: c1: 40, c2: 41\n",
    "# DISTANCES between O(BAS) and H(CH3)\n",
    "o4h7: DISTANCE ATOMS=34,46\n",
    "o4h4: DISTANCE ATOMS=34,43\n",
    "o4h5: DISTANCE ATOMS=34,44\n",
    "\n",
    "o2h7: DISTANCE ATOMS=22,46\n",
    "o2h4: DISTANCE ATOMS=22,43\n",
    "o2h5: DISTANCE ATOMS=22,44\n",
    "\n",
    "o3h7: DISTANCE ATOMS=26,46\n",
    "o3h4: DISTANCE ATOMS=26,43\n",
    "o3h5: DISTANCE ATOMS=26,44\n",
    "\n",
    "o1h7: DISTANCE ATOMS=17,46\n",
    "o1h4: DISTANCE ATOMS=17,43\n",
    "o1h5: DISTANCE ATOMS=17,44\n",
    "\n",
    "# DISTANCES between O(BAS) and H(CH2)\n",
    "o4h3: DISTANCE ATOMS=34,42\n",
    "o4h6: DISTANCE ATOMS=34,45\n",
    "\n",
    "o2h3: DISTANCE ATOMS=22,42\n",
    "o2h6: DISTANCE ATOMS=22,45\n",
    "\n",
    "o3h3: DISTANCE ATOMS=26,42\n",
    "o3h6: DISTANCE ATOMS=26,45\n",
    "\n",
    "o1h3: DISTANCE ATOMS=17,42\n",
    "o1h6: DISTANCE ATOMS=17,45\n",
    "\n",
    "# DISTANCES between O(BAS) and H(OH)\n",
    "o4h2: DISTANCE ATOMS=34,39\n",
    "o2h2: DISTANCE ATOMS=22,39\n",
    "o3h2: DISTANCE ATOMS=26,39\n",
    "o1h2: DISTANCE ATOMS=17,39\n",
    "\n",
    "# DISTANCES between O(BAS) and H(BAS)\n",
    "o4h1: DISTANCE ATOMS=34,37\n",
    "o2h1: DISTANCE ATOMS=22,37\n",
    "o3h1: DISTANCE ATOMS=26,37\n",
    "o1h1: DISTANCE ATOMS=17,37\n",
    "\n",
    "# DISTANCES between O(MeOH) and C\n",
    "o5c1: DISTANCE ATOMS=38,40\n",
    "o5c2: DISTANCE ATOMS=38,41\n",
    "\n",
    "# DISTANCES between O(MeOH) and H(CH3)\n",
    "o5h7: DISTANCE ATOMS=38,46\n",
    "o5h4: DISTANCE ATOMS=38,43\n",
    "o5h5: DISTANCE ATOMS=38,44\n",
    "\n",
    "# DISTANCES between O(MeOH) and H(CH2)\n",
    "o5h3: DISTANCE ATOMS=38,42\n",
    "o5h6: DISTANCE ATOMS=38,45\n",
    "\n",
    "# DISTANCES between O(MeOH) and H(O)\n",
    "o5h1: DISTANCE ATOMS=38,37\n",
    "o5h2: DISTANCE ATOMS=38,39\n",
    "\n",
    "\n",
    "# DISTANCE between atom 7 and 38\n",
    "d17: DISTANCE ATOMS=7,38\n",
    "\n",
    "# Apply upper wall to the distance between 7 and 38\n",
    "uwall: UPPER_WALLS ARG=d17 AT=3.5 KAPPA=200.0\n",
    "cv: PYTORCH_MODEL FILE={model_name} ARG=o4h7,o4h4,o4h5,o2h7,o2h4,o2h5,o3h7,o3h4,o3h5,o1h7,o1h4,o1h5,o4h3,o4h6,o2h3,o2h6,o3h3,o3h6,o1h3,o1h6,o4h2,o2h2,o3h2,o1h2,o4h1,o2h1,o3h1,o1h1,o5c1,o5c2,o5h7,o5h4,o5h5,o5h3,o5h6,o5h1,o5h2\n",
    "\n",
    "# UPPER_WALLS ARG=c1c2 AT=+8.5 KAPPA=250.0 EXP=2 LABEL=constr_c1c2 # Wall for potential constraints\n",
    "    '''\n",
    "    print(input, file=file)\n",
    "    file.close()\n",
    "    walltype=\"\"\n",
    "    if skew < 0:\n",
    "        walltype = \"UPPER_WALLS\"\n",
    "        offset = -offset\n",
    "    else:\n",
    "        walltype = \"LOWER_WALLS\"\n",
    "    with open(file_path,\"a\") as f:\n",
    "        print(f\"\"\"\n",
    "# Energy wall for aes cv\n",
    "wall: {walltype} ARG=cv.node-0 AT={pos+offset} KAPPA={kappa} ExP=2 EPS=1 OFFSET=0.0\n",
    "PRINT FMT=%g STRIDE=10 FILE={simulation_folder}/COLVAR ARG=o4h7,o4h4,o4h5,o2h7,o2h4,o2h5,o3h7,o3h4,o3h5,o1h7,o1h4,o1h5,o4h3,o4h6,o2h3,o2h6,o3h3,o3h6,o1h3,o1h6,o4h2,o2h2,o3h2,o1h2,o4h1,o2h1,o3h1,o1h1,o5c1,o5c2,o5h7,o5h4,o5h5,o5h3,o5h6,o5h1,o5h2,cv.*\"\"\",file=f)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def chaba_simulation(iter_folder, model_name, model, dataset, kappa, offset):\n",
    "    nn_output = cv_eval(model, dataset).flatten()\n",
    "    mu_sknn = np.mean(nn_output)\n",
    "    var_sknn = np.var(nn_output)\n",
    "    skew_sknn = stats.skew(nn_output)\n",
    "    offset += np.sqrt(var_sknn)\n",
    "    gen_plumed_chaba_biased(model_name=model_name,\n",
    "                         file_path=\".\",\n",
    "                         simulation_folder=iter_folder,\n",
    "                         pos=mu_sknn,\n",
    "                         skew=skew_sknn,\n",
    "                         kappa=kappa,\n",
    "                         offset=offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125cfdaa",
   "metadata": {},
   "source": [
    "### 3. Main Workflow\n",
    "\n",
    "Now we can start our main work flow.\n",
    "\n",
    "**1. Parameter initialization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0ce522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User customized parameters\n",
    "kappa = 500\n",
    "n_max_iter = 8\n",
    "loss_coeff = 0.1\n",
    "batch_size = 100\n",
    "offset = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a52995",
   "metadata": {},
   "source": [
    "Optional: Torch seed must be fixed to the following value in order to reproduce the results of the preprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad2fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c9bc67",
   "metadata": {},
   "source": [
    "**2. Clear the history data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([*bash_prefix,f\"rm -rf {RESULTS_FOLDER}\"], cwd=SCRIPT_DIR)\n",
    "subprocess.run([*bash_prefix,f\"rm -rf {LIGHTNING_LOGS}\"], cwd=SCRIPT_DIR)\n",
    "subprocess.run([*bash_prefix,f\"rm -rf {UNBIASED_FOLDER}\"], cwd=SCRIPT_DIR)\n",
    "subprocess.run([*bash_prefix, f\"rm -f {kappa}_iter* all*.pdb\"], cwd=SCRIPT_DIR)\n",
    "\n",
    "subprocess.run([*bash_prefix, f\"mkdir -p {UNBIASED_FOLDER}\"])\n",
    "\n",
    "subprocess.run([*bash_prefix, f\"echo '******************************************************'\"], cwd=SCRIPT_DIR)\n",
    "subprocess.run([*bash_prefix, f\"echo Start unbiased simulation\"], cwd=SCRIPT_DIR)\n",
    "subprocess.run([*bash_prefix, f\"echo '******************************************************'\"], cwd=SCRIPT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1354780",
   "metadata": {},
   "source": [
    "**3. Generate PLUMED input file for unbiased simulation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0702152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_plumed_chaba_unbiased()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05df718",
   "metadata": {},
   "source": [
    "**4. Run unbiased simulation using CP2K.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae7c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp2k.popt job.inp > output.log\n",
    "\n",
    "!echo \"Unbiased simulation finished\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f37bf",
   "metadata": {},
   "source": [
    "**5. Organizing data for further usage.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([*bash_prefix, \"mv Chaba-1.restart newiter.restart\"], cwd = SCRIPT_DIR)\n",
    "subprocess.run([*bash_prefix, \"rm -f Chaba*.restart\"], cwd=SCRIPT_DIR)\n",
    "subprocess.run([*bash_prefix, f\"mv Chaba-pos-1.pdb {kappa}_iteration_Chaba_unbiased-pos.pdb\"], cwd = SCRIPT_DIR)\n",
    "subprocess.run([*bash_prefix, f\"cat {kappa}_iteration_Chaba_unbiased-pos.pdb > all_{kappa}.pdb\"], cwd=SCRIPT_DIR)\n",
    "subprocess.run([*bash_prefix,\"rm -f PLUMED.OUT Chaba*\"], cwd=SCRIPT_DIR)\n",
    "subprocess.run([*bash_prefix, f\"mkdir -p {RESULTS_FOLDER}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11f6324",
   "metadata": {},
   "source": [
    "**6. Parse the initial unbiased sampling for automatically determining input descriptors.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4096424",
   "metadata": {},
   "outputs": [],
   "source": [
    "bond_type_dict, n_descriptors,heavy_atom_pairs_list = STADECT.parse_unbiased_colvar(colvar_file = f\"{UNBIASED_FOLDER}/COLVAR\")\n",
    "encoder_layers = [n_descriptors, 90, 40, 20, 5, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a01173",
   "metadata": {},
   "source": [
    "**7. Initialize state detection object.**\n",
    "\n",
    "   This object will save the current and historical states traversed by the system during the simulation.\n",
    "   \n",
    "   The initial state must be stable. Ohterwise further thermalization is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7408ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_detection = STADECT.State_detection((0.3, 0.7), bond_type_dict=bond_type_dict, n_heavy_atom_pairs=n_descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58c1aec",
   "metadata": {},
   "source": [
    "### Biased Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01de1662",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(n_max_iter):\n",
    "    # 1. Train the current model\n",
    "    state_detection, model, ITER_FOLDER, skewness_dataset, break_flag = chaba_training(state_detection, iter, encoder_layers, loss_coeff, batch_size)\n",
    "    # For logging\n",
    "    subprocess.run([*bash_prefix, f\"echo '******************************************************'\"], cwd=SCRIPT_DIR)\n",
    "    subprocess.run([*bash_prefix, f\"echo At the iteration {iter} training step, \"], cwd=SCRIPT_DIR)\n",
    "    subprocess.run([*bash_prefix, f\"echo The current state is {state_detection.current_state}\"], cwd=SCRIPT_DIR)\n",
    "    subprocess.run([*bash_prefix, f\"echo '******************************************************'\"], cwd=SCRIPT_DIR)\n",
    "\n",
    "    # 2. Generate PLUMED input files for biased simulation\n",
    "    model_name = f\"{ITER_FOLDER}/model_autoencoder_{iter}.pt\"\n",
    "    chaba_simulation(ITER_FOLDER, model_name, model, skewness_dataset, kappa, offset)\n",
    "    \n",
    "    # 3. Run Biased Simulation\n",
    "    subprocess.run([*bash_prefix,\"cp2k.popt job_restart.inp > output.log\"], cwd=SCRIPT_DIR)\n",
    "\n",
    "    # Organize trajectories\n",
    "    subprocess.run([*bash_prefix, \"mv Chaba-1.restart newiter.restart\"], cwd = SCRIPT_DIR)\n",
    "    subprocess.run([*bash_prefix, \"rm -f Chaba*.restart\"], cwd=SCRIPT_DIR)\n",
    "    subprocess.run([*bash_prefix, f\"mv Chaba-pos-1.pdb {kappa}_iteration_Chaba_{iter}-pos.pdb\"], cwd = SCRIPT_DIR)\n",
    "    subprocess.run([*bash_prefix, f\"tail -n +3 {kappa}_iteration_Chaba_{iter}-pos.pdb >> all_{kappa}.pdb\"], cwd=SCRIPT_DIR)\n",
    "    subprocess.run([*bash_prefix,\"rm -f PLUMED.OUT Chaba*\"], cwd=SCRIPT_DIR)\n",
    "\n",
    "    # logging\n",
    "    subprocess.run([*bash_prefix, f\"echo CP2K simulation at iteration {iter} with plumed ends\"], cwd=SCRIPT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tutorial4loxodynamics)",
   "language": "python",
   "name": "tutorial4loxodynamics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
